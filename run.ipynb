{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7f3111c-5849-4705-8e07-8a2f67999c82",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ASTE_dataloader_0503'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, WeightedRandomSampler\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdamW\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mASTE_dataloader_0503\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ASTE_Dataset, ASTE_collate_fn, load_vocab\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscheme\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspan_tagging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m form_label_id_map, form_sentiment_id_map\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluate_0503\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate_model, print_evaluate_dict\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ASTE_dataloader_0503'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from ASTE_dataloader_0503 import ASTE_Dataset, ASTE_collate_fn, load_vocab\n",
    "from scheme.span_tagging import form_label_id_map, form_sentiment_id_map\n",
    "from evaluate_0503 import evaluate_model, print_evaluate_dict\n",
    "\n",
    "from utils import create_logger, ensure_dir, set_seed\n",
    "\n",
    "from models.model_0509 import base_model\n",
    "import wandb\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def totally_parameters(model):\n",
    "    n_params = sum([p.nelement() for p in model.parameters()])\n",
    "    return n_params\n",
    "\n",
    "\n",
    "def form_weight_n(n):\n",
    "    if n > 6:\n",
    "        weight = torch.ones(n)\n",
    "        index_range = torch.tensor(range(n))\n",
    "        weight = weight + ((index_range & 3) > 0)\n",
    "    else:\n",
    "        weight = torch.tensor([1.0, 2.0, 2.0, 2.0, 1.0, 1.0])\n",
    "\n",
    "    return weight\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29bcb31-1208-4cf4-b630-1ba05ff45476",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--device', type=str, default='cuda', help='cuda,cpu')\n",
    "    parser.add_argument('--dataset_dir', type=str, default='./data/ASTE-Data-V2-EMNLP2020')\n",
    "    parser.add_argument('--saved_dir', type=str, default='saved_models')\n",
    "    parser.add_argument('--saved_file', type=str, default=None)\n",
    "    parser.add_argument('--pretrained_model', type=str, default='bert-base-uncased')\n",
    "    \n",
    "    parser.add_argument('--dataset', type=str, default='14res')\n",
    "    parser.add_argument('--model_name', type=str, default='TransformerModel')\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "    # parser.add_argument('--seed', type=int, help='Random seed for reproducibility')\n",
    "\n",
    "    parser.add_argument('--hidden_dim', type=int, default=128)\n",
    "    parser.add_argument('--num_epoch', type=int, default=80,help=\"80,100,120\")\n",
    "    parser.add_argument('--batch_size', type=int, default=16)\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=1e-5)\n",
    "    parser.add_argument('--dropout_rate', type=float, default=0.5)\n",
    "    parser.add_argument('--adam_epsilon', default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "\n",
    "    # loss\n",
    "    parser.add_argument('--with_weight', default=True, action='store_true')\n",
    "    parser.add_argument('--span_average', default=False, action='store_true')\n",
    "\n",
    "\n",
    "    #\n",
    "    args = parser.parse_args()\n",
    "    device = torch.device(args.device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    destination_folder = os.path.join(\"saved_files_results\", args.dataset_dir, args.dataset, args.version)\n",
    "    args.destination_folder = destination_folder\n",
    "    logger = create_logger(args, destination_folder)\n",
    "    logger.info('-' * 50)\n",
    "    logger.info(__file__)\n",
    "\n",
    "\n",
    "    if args.seed is None:\n",
    "        args.seed = random.randint(0, 2 ** 10 - 1)\n",
    "    set_seed(args.seed)\n",
    "\n",
    "\n",
    "    # 将所有参数及其实际使用的值记录到logger中\n",
    "    for arg in vars(args):\n",
    "        logger.info(f\"Argument {arg}: {getattr(args, arg)}\")\n",
    "    logger.info('-' * 50)\n",
    "\n",
    "    #############################################################################################\n",
    "    # train_and_evaluate(base_model, args)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.pretrained_model)\n",
    " \n",
    "\n",
    "    dataset_dir = args.dataset_dir + '/' + args.dataset\n",
    "    saved_dir = args.saved_dir + '/' + args.dataset\n",
    "    ensure_dir(saved_dir)\n",
    "\n",
    "    # 加载对应词汇表\n",
    "    vocab = load_vocab(dataset_dir=dataset_dir)\n",
    "    # 标签映射id与标签，维度转换函数\n",
    "    label2id, id2label = form_label_id_map(args.version)\n",
    "    senti2id, id2senti = form_sentiment_id_map()\n",
    "    # 将标签和情感的映射信息添加到词汇表中\n",
    "    vocab['label_vocab'] = dict(label2id=label2id, id2label=id2label)\n",
    "    vocab['senti_vocab'] = dict(senti2id=senti2id, id2senti=id2senti)\n",
    "\n",
    "\n",
    "    logger.info(\"Load dataset...\")\n",
    "\n",
    "    train_dataset = ASTE_Dataset(file_name=os.path.join(dataset_dir, 'train_triplets.txt'),\n",
    "                                 version=args.version,\n",
    "                                 vocab=vocab,\n",
    "                                 tokenizer=tokenizer)\n",
    "    valid_dataset = ASTE_Dataset(file_name=os.path.join(dataset_dir, 'dev_triplets.txt'),\n",
    "                                 version=args.version,\n",
    "                                 vocab=vocab,\n",
    "                                 tokenizer=tokenizer)\n",
    "    test_dataset = ASTE_Dataset(file_name=os.path.join(dataset_dir, 'test_triplets.txt'),\n",
    "                                version=args.version,\n",
    "                                vocab=vocab,\n",
    "                                tokenizer=tokenizer)\n",
    "\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, collate_fn=ASTE_collate_fn, shuffle=False)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=args.batch_size, collate_fn=ASTE_collate_fn, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, collate_fn=ASTE_collate_fn, shuffle=False)\n",
    "\n",
    "\n",
    "###############################################\n",
    "\n",
    "    # 参数设置：class_n--类别数量，weight--权重向量，\n",
    "    class_n = len(label2id)\n",
    "    args.class_n = class_n\n",
    "    weight = form_weight_n(class_n).to(args.device) if args.with_weight else None\n",
    "\n",
    "    logger.info(\"label2id:{}\".format(label2id))\n",
    "    logger.info(\"weight:{}\".format(weight))\n",
    "\n",
    "    logger.info(\"Load models...\")\n",
    "\n",
    "    model = base_model(model_name=args.model_name,\n",
    "                       pretrained_model_path=args.pretrained_model,\n",
    "                       hidden_dim=args.hidden_dim,\n",
    "                       dropout=args.dropout_rate,\n",
    "                       class_n=class_n,\n",
    "                       span_average=args.span_average\n",
    "                       ).to(args.device)\n",
    "\n",
    "    logger.info(\"# parameters:{}\".format(totally_parameters(model)))\n",
    "\n",
    "    \n",
    "    # optimizer = AdamW(model.parameters(),lr=args.lr,weight_decay=1e-4)\n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr)\n",
    "\n",
    "    best_f1 = 0\n",
    "    stop_count = 0\n",
    "    increase_count = 0\n",
    "    last_loss = float('inf')  # 初始化\n",
    "\n",
    "    logger.info(\"Training...\")\n",
    "    \n",
    "    for epoch in range(1, args.num_epoch + 1):\n",
    "\n",
    "        train_loss = 0.\n",
    "        total_step = 0\n",
    "\n",
    "        epoch_begin = time.time()\n",
    "\n",
    "        # 模型训练：设置训练模式；清除优化器的梯度；将批次数据移动到指定设备；前向传播计算损失；累积训练损失和步骤；反向传播计算梯度；优化器更新模型参数。\n",
    "        for batch in train_dataloader:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = {k: v.to(args.device) for k, v in batch.items()}\n",
    "            outputs = model(inputs, weight)\n",
    "            \n",
    "            loss = outputs['loss']\n",
    "            total_step += 1\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 评估模型\n",
    "        valid_loss, valid_results = evaluate_model(model, valid_dataset, valid_dataloader,\n",
    "                                                   id2senti=id2senti,\n",
    "                                                   device=args.device,\n",
    "                                                   version=args.version,\n",
    "                                                   weight=weight)\n",
    "\n",
    "        triplet_f1 = 100.0 * valid_results[0]['triplet']['f1']\n",
    "        logger.info('Epoch:{}/{} \\ttrain_loss:{:.4f}\\tvalid_loss:{:.4f}\\ttriplet_f1:{:.4f}% [{:.4f}s]'\n",
    "                    .format(epoch, args.num_epoch, train_loss / total_step, valid_loss,\n",
    "                           triplet_f1 , time.time() - epoch_begin))\n",
    "\n",
    "        \n",
    "#######################################################################################################\n",
    "    logger.info('> Testing...')\n",
    "    # models performance on the test set\n",
    "    _, test_results = evaluate_model(model, test_dataset, test_dataloader,\n",
    "                                     id2senti=id2senti,\n",
    "                                     device=args.device,\n",
    "                                     version=args.version,\n",
    "                                     weight=weight)\n",
    "\n",
    "\n",
    "    logger.info('-' * 50)\n",
    "    logger.info('Dataset:{}, test_f1:{:.2f}% | version:{} lr:{} seed:{} dropout:{}'\n",
    "                .format(args.dataset, test_results[0]['triplet']['f1'] * 100, args.version, args.lr,\n",
    "                        args.seed, args.dropout_rate))\n",
    "    evaluate_result = print_evaluate_dict(test_results)\n",
    "    logger.info(\"evaluate result：\\n{}\".format(evaluate_result))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
